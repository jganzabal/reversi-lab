{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f36c9e46-f457-4944-b6a0-c5840d49c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1ea8d3-441b-46c9-83fe-181fda480d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import make_reversi_vec_env, SelfPlayEnv\n",
    "from stable_baselines3 import PPO\n",
    "from models import CustomActorCriticPolicy\n",
    "from players import RandomPlayer\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bedc7fb-68ba-4b93-b6c7-c791ce5c309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 8\n",
    "gamma = 0.99\n",
    "ent_coef = 0.0\n",
    "gae_lambda = 0.95\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e99e4ec-d16c-47af-945c-cf840a446a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=1,\n",
    "    env_kwargs={'board_shape': board_shape, 'Player': RandomPlayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dca388f-838f-4f32-9dbc-9ec40b4d6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO_8by8_0.99_0.95_0.0_10_8_masked\n",
      "./models/PPO_8by8_0.99_0.95_0.0_10_8_masked\n"
     ]
    }
   ],
   "source": [
    "model_name = f'PPO_{board_shape}by{board_shape}_{gamma}_{gae_lambda}_{ent_coef}_{n_epochs}_{n_envs}_masked'\n",
    "best_model_save_path = f'./models/{model_name}'\n",
    "print(model_name)\n",
    "print(best_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0247c658-a411-403c-9a5c-5b1540eb6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={'board_shape': board_shape, 'Player': RandomPlayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cce2c7-9a08-40b1-bc39-27d614edfdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jupyter-julian/.cache/torch/hub/pytorch_vision_v0.9.0\n"
     ]
    }
   ],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicy,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log='../reversi/testing/',\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    ent_coef=ent_coef,\n",
    "    n_epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d67fe8f-7fde-428f-b494-c3690f1afb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(board):\n",
    "    return eval_env.envs[0].get_valid((board, 1)).reshape(-1)  \n",
    "model.policy.set_get_valid_actions(get_valid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0faa893b-9538-4260-a408-30c78327a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env = eval_env,\n",
    "    eval_freq=5_000,\n",
    "    n_eval_episodes=500,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af3005-d349-4b0c-814c-b0cccc63b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=0.42 +/- 0.89\n",
      "Episode length: 30.07 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 30.11 +/- 1.11\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.07 +/- 0.83\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 29.96 +/- 1.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=0.67 +/- 0.71\n",
      "Episode length: 30.02 +/- 1.23\n",
      "Eval num_timesteps=240000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 30.08 +/- 1.27\n",
      "Eval num_timesteps=280000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 30.05 +/- 0.68\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=0.77 +/- 0.63\n",
      "Episode length: 30.02 +/- 0.99\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=0.73 +/- 0.68\n",
      "Episode length: 30.08 +/- 1.12\n",
      "Eval num_timesteps=400000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 29.97 +/- 1.24\n",
      "Eval num_timesteps=440000, episode_reward=0.73 +/- 0.67\n",
      "Episode length: 29.96 +/- 1.24\n",
      "Eval num_timesteps=480000, episode_reward=0.74 +/- 0.65\n",
      "Episode length: 29.96 +/- 1.79\n",
      "Eval num_timesteps=520000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 30.09 +/- 1.69\n",
      "Eval num_timesteps=560000, episode_reward=0.52 +/- 0.84\n",
      "Episode length: 29.97 +/- 1.29\n",
      "Eval num_timesteps=600000, episode_reward=0.53 +/- 0.84\n",
      "Episode length: 30.02 +/- 1.41\n",
      "Eval num_timesteps=640000, episode_reward=0.56 +/- 0.82\n",
      "Episode length: 30.07 +/- 1.18\n",
      "Eval num_timesteps=680000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.99 +/- 1.63\n",
      "Eval num_timesteps=720000, episode_reward=0.57 +/- 0.81\n",
      "Episode length: 29.99 +/- 0.70\n",
      "Eval num_timesteps=760000, episode_reward=0.45 +/- 0.88\n",
      "Episode length: 29.86 +/- 1.24\n",
      "Eval num_timesteps=800000, episode_reward=0.50 +/- 0.85\n",
      "Episode length: 30.01 +/- 1.18\n",
      "Eval num_timesteps=840000, episode_reward=0.20 +/- 0.96\n",
      "Episode length: 29.92 +/- 0.65\n",
      "Eval num_timesteps=880000, episode_reward=0.33 +/- 0.93\n",
      "Episode length: 30.00 +/- 0.71\n",
      "Eval num_timesteps=920000, episode_reward=0.37 +/- 0.90\n",
      "Episode length: 29.98 +/- 0.91\n",
      "Eval num_timesteps=960000, episode_reward=0.41 +/- 0.89\n",
      "Episode length: 30.00 +/- 0.77\n",
      "Eval num_timesteps=1000000, episode_reward=0.26 +/- 0.95\n",
      "Episode length: 29.92 +/- 0.69\n",
      "Eval num_timesteps=1040000, episode_reward=0.22 +/- 0.96\n",
      "Episode length: 29.96 +/- 0.72\n",
      "Eval num_timesteps=1080000, episode_reward=0.05 +/- 0.98\n",
      "Episode length: 29.87 +/- 0.75\n",
      "Eval num_timesteps=1120000, episode_reward=0.10 +/- 0.98\n",
      "Episode length: 29.89 +/- 0.64\n",
      "Eval num_timesteps=1160000, episode_reward=0.18 +/- 0.97\n",
      "Episode length: 29.85 +/- 1.44\n",
      "Eval num_timesteps=1200000, episode_reward=0.10 +/- 0.98\n",
      "Episode length: 29.89 +/- 1.06\n",
      "Eval num_timesteps=1240000, episode_reward=-0.03 +/- 0.99\n",
      "Episode length: 29.85 +/- 1.39\n",
      "Eval num_timesteps=1280000, episode_reward=0.11 +/- 0.98\n",
      "Episode length: 29.96 +/- 0.70\n",
      "Eval num_timesteps=1320000, episode_reward=0.14 +/- 0.97\n",
      "Episode length: 29.94 +/- 1.08\n",
      "Eval num_timesteps=1360000, episode_reward=0.19 +/- 0.97\n",
      "Episode length: 29.88 +/- 1.11\n",
      "Eval num_timesteps=1400000, episode_reward=0.00 +/- 0.98\n",
      "Episode length: 29.89 +/- 0.62\n",
      "Eval num_timesteps=1440000, episode_reward=0.28 +/- 0.95\n",
      "Episode length: 29.97 +/- 0.68\n",
      "Eval num_timesteps=1480000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 29.93 +/- 1.23\n",
      "Eval num_timesteps=1520000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 29.98 +/- 0.61\n",
      "Eval num_timesteps=1560000, episode_reward=0.45 +/- 0.88\n",
      "Episode length: 29.94 +/- 1.01\n",
      "Eval num_timesteps=1600000, episode_reward=0.65 +/- 0.74\n",
      "Episode length: 29.96 +/- 1.16\n",
      "Eval num_timesteps=1640000, episode_reward=0.61 +/- 0.78\n",
      "Episode length: 29.80 +/- 1.88\n",
      "Eval num_timesteps=1680000, episode_reward=0.62 +/- 0.76\n",
      "Episode length: 29.88 +/- 1.26\n",
      "Eval num_timesteps=1720000, episode_reward=0.54 +/- 0.83\n",
      "Episode length: 29.95 +/- 0.62\n",
      "Eval num_timesteps=1760000, episode_reward=0.45 +/- 0.87\n",
      "Episode length: 29.86 +/- 1.25\n",
      "Eval num_timesteps=1800000, episode_reward=0.49 +/- 0.86\n",
      "Episode length: 29.80 +/- 1.43\n",
      "Eval num_timesteps=1840000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.91 +/- 0.64\n",
      "Eval num_timesteps=1880000, episode_reward=0.54 +/- 0.84\n",
      "Episode length: 29.88 +/- 0.70\n",
      "Eval num_timesteps=1920000, episode_reward=0.54 +/- 0.82\n",
      "Episode length: 29.96 +/- 0.62\n",
      "Eval num_timesteps=1960000, episode_reward=0.57 +/- 0.82\n",
      "Episode length: 29.98 +/- 0.67\n",
      "Eval num_timesteps=2000000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 29.86 +/- 1.18\n",
      "Eval num_timesteps=2040000, episode_reward=0.53 +/- 0.82\n",
      "Episode length: 29.80 +/- 1.05\n",
      "Eval num_timesteps=2080000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 29.91 +/- 1.41\n",
      "Eval num_timesteps=2120000, episode_reward=0.58 +/- 0.78\n",
      "Episode length: 29.96 +/- 0.61\n",
      "Eval num_timesteps=2160000, episode_reward=0.60 +/- 0.79\n",
      "Episode length: 29.93 +/- 0.64\n",
      "Eval num_timesteps=2200000, episode_reward=0.69 +/- 0.72\n",
      "Episode length: 29.96 +/- 1.23\n",
      "Eval num_timesteps=2240000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 29.89 +/- 1.13\n",
      "Eval num_timesteps=2280000, episode_reward=0.65 +/- 0.75\n",
      "Episode length: 29.90 +/- 1.59\n",
      "Eval num_timesteps=2320000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 29.95 +/- 0.62\n",
      "Eval num_timesteps=2360000, episode_reward=0.59 +/- 0.80\n",
      "Episode length: 29.95 +/- 1.28\n",
      "Eval num_timesteps=2400000, episode_reward=0.58 +/- 0.80\n",
      "Episode length: 30.03 +/- 0.68\n",
      "Eval num_timesteps=2440000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.02 +/- 0.96\n",
      "Eval num_timesteps=2480000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.00 +/- 0.79\n",
      "Eval num_timesteps=2520000, episode_reward=0.68 +/- 0.71\n",
      "Episode length: 30.02 +/- 0.61\n",
      "Eval num_timesteps=2560000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 29.91 +/- 1.22\n",
      "Eval num_timesteps=2600000, episode_reward=0.59 +/- 0.79\n",
      "Episode length: 29.87 +/- 1.26\n",
      "Eval num_timesteps=2640000, episode_reward=0.72 +/- 0.69\n",
      "Episode length: 29.95 +/- 0.58\n",
      "Eval num_timesteps=2680000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.04 +/- 0.65\n",
      "Eval num_timesteps=2720000, episode_reward=0.69 +/- 0.70\n",
      "Episode length: 29.86 +/- 1.41\n",
      "Eval num_timesteps=2760000, episode_reward=0.64 +/- 0.75\n",
      "Episode length: 29.99 +/- 0.64\n",
      "Eval num_timesteps=2800000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 29.94 +/- 0.60\n",
      "Eval num_timesteps=2840000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 29.91 +/- 1.62\n",
      "Eval num_timesteps=2880000, episode_reward=0.62 +/- 0.77\n",
      "Episode length: 29.90 +/- 1.38\n",
      "Eval num_timesteps=2920000, episode_reward=0.66 +/- 0.73\n",
      "Episode length: 29.98 +/- 0.63\n",
      "Eval num_timesteps=2960000, episode_reward=0.63 +/- 0.75\n",
      "Episode length: 29.96 +/- 1.27\n",
      "Eval num_timesteps=3000000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 29.90 +/- 1.60\n",
      "Eval num_timesteps=3040000, episode_reward=0.76 +/- 0.63\n",
      "Episode length: 29.98 +/- 1.14\n",
      "Eval num_timesteps=3080000, episode_reward=0.71 +/- 0.69\n",
      "Episode length: 30.03 +/- 0.63\n",
      "Eval num_timesteps=3120000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 29.94 +/- 1.19\n",
      "Eval num_timesteps=3160000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 30.03 +/- 0.59\n",
      "Eval num_timesteps=3200000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 29.96 +/- 1.18\n",
      "Eval num_timesteps=3240000, episode_reward=0.67 +/- 0.72\n",
      "Episode length: 30.01 +/- 0.63\n",
      "Eval num_timesteps=3280000, episode_reward=0.60 +/- 0.78\n",
      "Episode length: 29.92 +/- 1.23\n",
      "Eval num_timesteps=3320000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 29.99 +/- 0.62\n",
      "Eval num_timesteps=3360000, episode_reward=0.64 +/- 0.76\n",
      "Episode length: 29.96 +/- 1.21\n",
      "Eval num_timesteps=3400000, episode_reward=0.67 +/- 0.73\n",
      "Episode length: 29.96 +/- 1.14\n",
      "Eval num_timesteps=3440000, episode_reward=0.76 +/- 0.63\n",
      "Episode length: 30.02 +/- 0.58\n",
      "Eval num_timesteps=3480000, episode_reward=0.70 +/- 0.70\n",
      "Episode length: 29.99 +/- 0.60\n",
      "Eval num_timesteps=3520000, episode_reward=0.77 +/- 0.62\n",
      "Episode length: 30.03 +/- 0.58\n",
      "Eval num_timesteps=3560000, episode_reward=0.77 +/- 0.62\n",
      "Episode length: 30.00 +/- 0.96\n",
      "Eval num_timesteps=3600000, episode_reward=0.75 +/- 0.65\n",
      "Episode length: 29.93 +/- 1.47\n",
      "Eval num_timesteps=3640000, episode_reward=0.78 +/- 0.60\n",
      "Episode length: 30.03 +/- 0.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3680000, episode_reward=0.74 +/- 0.66\n",
      "Episode length: 30.00 +/- 1.07\n",
      "Eval num_timesteps=3720000, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 30.04 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3760000, episode_reward=0.76 +/- 0.64\n",
      "Episode length: 30.03 +/- 0.56\n",
      "Eval num_timesteps=3800000, episode_reward=0.68 +/- 0.72\n",
      "Episode length: 30.00 +/- 0.56\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e10), tb_log_name=model_name, callback=[eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff089b-103b-4df7-a7c4-960838879f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
