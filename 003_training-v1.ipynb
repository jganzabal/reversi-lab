{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36c9e46-f457-4944-b6a0-c5840d49c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b1ea8d3-441b-46c9-83fe-181fda480d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import make_reversi_vec_env, SelfPlayEnv\n",
    "from stable_baselines3 import PPO\n",
    "from models import CustomActorCriticPolicyMLP\n",
    "from players import RandomPlayer\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bedc7fb-68ba-4b93-b6c7-c791ce5c309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "board_shape = 8\n",
    "n_envs = 8\n",
    "gamma = 0.99\n",
    "ent_coef = 0.0\n",
    "gae_lambda = 0.95\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f8f6d3a-5ff5-41a3-a5f9-d0ac892bd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_player_eval = RandomPlayer(board_shape=board_shape, flatten_action=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8555e92a-2a49-4d3b-91eb-e0069bba24ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=1,\n",
    "    env_kwargs={'board_shape': board_shape, 'LocalPlayer': RandomPlayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61a8e37c-cc24-45cb-8f4b-c01f816f70e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dca388f-838f-4f32-9dbc-9ec40b4d6de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO_8by8_0.99_0.95_0.0_10_8_masked_mlp\n",
      "./models/PPO_8by8_0.99_0.95_0.0_10_8_masked_mlp\n"
     ]
    }
   ],
   "source": [
    "model_name = f'PPO_{board_shape}by{board_shape}_{gamma}_{gae_lambda}_{ent_coef}_{n_epochs}_{n_envs}_masked_mlp'\n",
    "best_model_save_path = f'./models/{model_name}'\n",
    "print(model_name)\n",
    "print(best_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a907c53-cb79-4d3a-a34a-6d464f808f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_player = RandomPlayer(board_shape=board_shape, flatten_action=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0247c658-a411-403c-9a5c-5b1540eb6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_reversi_vec_env(\n",
    "    SelfPlayEnv, n_envs=n_envs,\n",
    "    env_kwargs={'board_shape': board_shape, 'LocalPlayer': RandomPlayer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03cce2c7-9a08-40b1-bc39-27d614edfdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    CustomActorCriticPolicyMLP,\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log='../reversi/testing/',\n",
    "    gamma=gamma,\n",
    "    gae_lambda=gae_lambda,\n",
    "    ent_coef=ent_coef,\n",
    "    n_epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91536ace-3c41-4caa-9356-f9e6fef18cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomActorCriticPolicyMLP(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): CustomNetwork(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d67fe8f-7fde-428f-b494-c3690f1afb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions(board):\n",
    "    return eval_env.envs[0].get_valid((board, 1)).reshape(-1)  \n",
    "model.policy.set_get_valid_actions(get_valid_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0faa893b-9538-4260-a408-30c78327a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_callback = EvalCallback(\n",
    "    eval_env = eval_env,\n",
    "    eval_freq=5_000,\n",
    "    n_eval_episodes=500,\n",
    "    deterministic=True,\n",
    "    verbose=1,\n",
    "    best_model_save_path=best_model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af3005-d349-4b0c-814c-b0cccc63b997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=40000, episode_reward=0.53 +/- 0.83\n",
      "Episode length: 29.89 +/- 1.03\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=0.61 +/- 0.77\n",
      "Episode length: 29.98 +/- 0.57\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=0.66 +/- 0.74\n",
      "Episode length: 29.86 +/- 1.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=0.72 +/- 0.67\n",
      "Episode length: 29.89 +/- 1.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=200000, episode_reward=0.69 +/- 0.71\n",
      "Episode length: 29.89 +/- 1.42\n",
      "Eval num_timesteps=240000, episode_reward=0.73 +/- 0.66\n",
      "Episode length: 29.81 +/- 1.47\n",
      "New best mean reward!\n",
      "Eval num_timesteps=280000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 29.93 +/- 0.61\n",
      "Eval num_timesteps=320000, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 29.92 +/- 0.55\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=0.78 +/- 0.60\n",
      "Episode length: 29.87 +/- 0.74\n",
      "Eval num_timesteps=400000, episode_reward=0.72 +/- 0.68\n",
      "Episode length: 29.83 +/- 1.21\n",
      "Eval num_timesteps=440000, episode_reward=0.78 +/- 0.60\n",
      "Episode length: 29.86 +/- 1.25\n",
      "Eval num_timesteps=480000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 29.84 +/- 1.15\n",
      "New best mean reward!\n",
      "Eval num_timesteps=520000, episode_reward=0.83 +/- 0.54\n",
      "Episode length: 29.81 +/- 1.52\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=0.78 +/- 0.60\n",
      "Episode length: 29.91 +/- 0.58\n",
      "Eval num_timesteps=600000, episode_reward=0.79 +/- 0.60\n",
      "Episode length: 29.85 +/- 0.54\n",
      "Eval num_timesteps=640000, episode_reward=0.80 +/- 0.60\n",
      "Episode length: 29.77 +/- 1.25\n",
      "Eval num_timesteps=680000, episode_reward=0.78 +/- 0.61\n",
      "Episode length: 29.85 +/- 0.96\n",
      "Eval num_timesteps=720000, episode_reward=0.77 +/- 0.62\n",
      "Episode length: 29.84 +/- 1.46\n",
      "Eval num_timesteps=760000, episode_reward=0.78 +/- 0.60\n",
      "Episode length: 29.83 +/- 1.03\n",
      "Eval num_timesteps=800000, episode_reward=0.77 +/- 0.61\n",
      "Episode length: 29.76 +/- 1.42\n",
      "Eval num_timesteps=840000, episode_reward=0.85 +/- 0.51\n",
      "Episode length: 29.80 +/- 1.10\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=0.80 +/- 0.58\n",
      "Episode length: 29.90 +/- 0.51\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=int(1e10), tb_log_name=model_name, callback=[eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff089b-103b-4df7-a7c4-960838879f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
